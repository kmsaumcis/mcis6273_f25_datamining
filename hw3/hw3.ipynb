{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCIS6273 Data Mining (Prof. Maull) / Fall 2025 / HW3\n",
    "\n",
    "\n",
    "| Points <br/>Possible | Due Date | Time Commitment <br/>(estimated) |\n",
    "|:---------------:|:--------:|:---------------:|\n",
    "| 16 | Wednesday November 12 @ Midnight | _up to_ 20 hours |\n",
    "\n",
    "\n",
    "* **GRADING:** Grading will be aligned with the completeness of the objectives.\n",
    "\n",
    "* **INDEPENDENT WORK:** Copying, cheating, plagiarism  and academic dishonesty _are not tolerated_ by University or course policy.  Please see the syllabus for the full departmental and University statement on the academic code of honor.\n",
    "\n",
    "## OBJECTIVES\n",
    "* Perform data imputation using K-Nearest Neighbors.\n",
    "\n",
    "* Cluster data from GaN light pollution data features using K-Means.\n",
    "\n",
    "* Visualize cluster data from GaN light pollution clustering.\n",
    "\n",
    "* Complete the online assessment.\n",
    "\n",
    "## WHAT TO TURN IN\n",
    "You are being encouraged to turn the assignment in using the provided\n",
    "Jupyter Notebook.  To do so, make a directory in your Lab environment called\n",
    "`homework/hw3`.   Put all of your files in that directory.  Then zip or tar that directory,\n",
    "rename it with your name as the first part of the filename \n",
    "(e.g. `maull_hw3_files.zip`, `maull_hw3_files.tar.gz`), then\n",
    "download it to your local machine, then upload the `.zip` to Blackboard.\n",
    "\n",
    "If you do not know how to do this, please ask, or visit one of the \n",
    "many tutorials out there\n",
    "on the basics of using zip in Linux.\n",
    "\n",
    "If you choose not to use the provided notebook, you will still need to turn in a\n",
    "`.ipynb` Jupyter Notebook and corresponding files according to the instructions in\n",
    "this homework.\n",
    "\n",
    "\n",
    "## ASSIGNMENT TASKS\n",
    "### (25%) Perform data imputation using K-Nearest Neighbors. \n",
    "\n",
    "As you noticed in doing HW2, if we look at the SQM measurements\n",
    "there are a lot of missing values.  In fact more than 60%\n",
    "of the data is missing SQM!  This is actually expected, since\n",
    "the original GaN data was never intended to focus on quantitative\n",
    "measurements.\n",
    "\n",
    "We will ultimately like to have this data, even if it is \n",
    "not completely accurate, because it might be less compelling\n",
    "to build predictive models based on a _smallish_ subset of data.\n",
    "\n",
    "One of the main problems we will face, though, is that the\n",
    "SQM measurements are not well distributed.  We're going to\n",
    "ignore that for a moment, since there's not much that can be \n",
    "done just yet about it.\n",
    "\n",
    "_Data imputation_ provides a mechanism for \"filling\" in \n",
    "this missing data.  There are a number of ways to do \n",
    "this -- one being to put in some random value for all missing \n",
    "values.  You \n",
    "might already realize that could be a sub-optimal \n",
    "thing to do.\n",
    "\n",
    "The true intuition behind data imputation is this:\n",
    "\n",
    "* given a set of missing values, fill in those missing values\n",
    "  based on some criteria,\n",
    "* sometimes we can just fill in missing values with the mean or median\n",
    "  of the entire dataset,\n",
    "* sometimes we'd like more sophistication, like in K-Nearest Neighbors\n",
    "  imputation.\n",
    "\n",
    "In K-Nearest Neighbors imputation, we look at features which are _not_ \n",
    "missing and look at similar data, compare _k_ of them which contain \n",
    "the absent features and fill in missing values from the _k_ means \n",
    "or medians.  This provides better approximations of missing data \n",
    "in large datasets -- it is akin to you deciding what color to paint \n",
    "your house based on the average of the colors of your\n",
    "adjacent _k_ neigbors' homes. \n",
    "\n",
    "This is a critical task to perform in a lot of data, especially\n",
    "when it is necessary to build models on as much data as possible.\n",
    "\n",
    "Study the SciKit-Learn modules:\n",
    "\n",
    "* [`KNNImputer`](https://scikit-learn.org/stable/modules/impute)\n",
    "\n",
    "**&#167; Task:**  **1.1 Impute the data using K-Nearest Neighbor imputer in SciKit-Learn.**\n",
    "\n",
    "Here you will remove all data except values which are `-1` and\n",
    "between `17` and and `23` (inclusive of each).  You will also remove any data\n",
    "which is duplicated using  `drop_duplicates()`.  It is recommended you\n",
    "do this **before** constraining the `SQMReading`s. \n",
    "\n",
    "\n",
    "**&#167; Task:**  **1.2 Provide the updated statistics for the completed dataset which now includes\n",
    "  original and imputed SQM values.**\n",
    "\n",
    "* report mean, median and standard deviation of the **new** imputed data.\n",
    "\n",
    "\n",
    "**&#167; Task:**  **1.3 Use Folium to plot the data for the bounding box and constraints given.**\n",
    "\n",
    "* `Latitude` > 45.79 and < 45.92 ,\n",
    "* `Longitude` > 15 and < 16,\n",
    "* `SQMReading` above 20,\n",
    "* Plot the original SQM Reading with red pins, plot the imputed data with blue pins,\n",
    "* Include the `Latitude`, `Longitude` and `SQMReading` in the popup of the pin.\n",
    "\n",
    "\n",
    "**&#167; Task:**  **BONUS: Implement your K-Nearest Neighbor imputation solution with only the `NearestNeighbors` algorithm of SciKit-Learn.**\n",
    "\n",
    "Here you will write you own imputer without the help of the built-in one.\n",
    "\n",
    "You can easily do this (and possibly improve the running time) by considering:\n",
    "\n",
    "* use SciKit-Learn's [`NearestNeighbors`](https://scikit-learn.org/stable/modules/neighbors.html&#35;unsupervised-nearest-neighbors) algorithm **not** the `KNNImputer`\n",
    "* compare nearest neighbors only by Latitude and Longitude (ignore that the distance Euclidean distance metric does not account for curvature \n",
    "  of the Earth, but is a good enough approximation for this dataset)\n",
    "* use the median of the 5 nearest neighbors \n",
    "* plot the Folium map for the same region as above\n",
    "\n",
    "This part can earn you up to 4 extra points!  Don't overthink this.  Only a few\n",
    "lines of code are necessary if cleverly thought out.  You may find\n",
    "using [`numpy`](https://numpy.org) will make things a little easier, then you may convert\n",
    "back to Pandas if you wish (though that is not necessary).\n",
    "\n",
    "\n",
    "\n",
    "### (50%) Cluster data from GaN light pollution data features using K-Means. \n",
    "\n",
    "In lecture, we talked about the value of clustering\n",
    "to understand the patterns in underlying data\n",
    "especially when there are a large number of \n",
    "instances and features.\n",
    "\n",
    "We are going to use the clustering algorithms\n",
    "in SciKit-Learn (for now K-Means) to uncover\n",
    "the broader patterns in the data.\n",
    "\n",
    "To do this we are going to complete a little\n",
    "housekeeping with our dataset so that we have\n",
    "all binary features.  Then we will cluster\n",
    "and visualize our data.\n",
    "\n",
    "**&#167; Task:**  **2.1 Convert `CloudCoverPct` and `LimitingMag` to binary features, eliminate all remaining non-binary features except `SQMReading`.**\n",
    "\n",
    "\n",
    "Perform `get_dummies()` as was done in HW1.\n",
    "\n",
    "You will need to create a final Dataframe with the following features:\n",
    "\n",
    "* `SQMReading >= 17 and < 23`,\n",
    "* `LimingMag` must be greater `0`,\n",
    "* retain the following **32** columns:  `['SQM Reading','const_Bootes', 'const_Canis Major', 'const_Crux', 'const_Cygnus',\n",
    "  'const_Gemini', 'const_Grus', 'const_Hercules', 'const_Leo',\n",
    "  'const_Orion', 'const_Pegasus', 'const_Perseus', 'const_Sagittarius',\n",
    "  'const_Scorpius', 'const_Taurus', 'loc_remote', 'loc_rural',\n",
    "  'loc_suburban', 'loc_unknown', 'loc_urban', 'ccp_0.0', 'ccp_0.25',\n",
    "  'ccp_0.5', 'ccp_0.75', 'lm_0', 'lm_1', 'lm_2', 'lm_3', 'lm_4', 'lm_5',\n",
    "  'lm_6', 'lm_7']` where `ccp` indicates the 4 levels of cloud cover percent\n",
    "  and `lm` indicates the 8 levels of limiting magnitude,\n",
    "* the non-binary features: `Latitude`, `Longitude`, `Elevation(m)`, `LocalDateTime` , `CloudCoverPct` and `LimitingMag` will be eliminated.\n",
    " \n",
    "An example data row might look like this:\n",
    "\n",
    "|    |   `SQM Reading` |   `const_Bootes` |   `const_Canis Major` |  ... | `loc_ remote` |   ... |  `loc_ suburban` |   `ccp_0.0` |   ... |   `lm_7` |\n",
    "|---:|----------------:|-----------------:|----------------------:|------|-------------:|-------|--------------:|----------:|------:|-------:|\n",
    "|  0 |         19.7    |                0 |                   0   |  ... |            0 |   ... |             0 |  1        |   ... |      0 |\n",
    "\n",
    "* save the Dataframe to a file called `gan_cluster_final.csv`.\n",
    "\n",
    "\n",
    "**&#167; Task:**  **2.2 Use [`sklearn.cluster.kmeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) to cluster the data in `gan_cluster_final.csv`.**\n",
    "\n",
    "Review the algorithm [here](https://scikit-learn.org/stable/modules/clustering.html&#35;k-means).\n",
    "\n",
    "* You will eliminate the `SQMReading` feature in your clustering,\n",
    "* set `n_init=1` and `n_clusters=12` and `random_state=0`,\n",
    "* use the label array that comes out of `kmeans` to assign \n",
    "  a new feature (column) called `cluster`,\n",
    "* create a new file that contains the cluster label to all datapoints and \n",
    "  store it along with the original data in  \n",
    "  `gan_cluster_final_assigned.csv` files, retaining the `SQMReading`.\n",
    "\n",
    "\n",
    "**&#167; Task:**  **2.3 Use [`groupby()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) to group by all the clusters.**\n",
    "\n",
    "Now that every observation (row) has a cluster assignment,\n",
    "we are going to group by those clusters.\n",
    "\n",
    "This will effectively \"add up\" all the feature contributions\n",
    "of each instance to the cluster.  Since they are all\n",
    "binary, this makes the math very easy to see and \n",
    "makes these contributions obvious and potentially useful.\n",
    "\n",
    "You will use [`.sum()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.sum.html&#35;pandas.core.groupby.DataFrameGroupBy.sum)\n",
    "on `groupby()` to aggregate the data -- just remember \n",
    "to use the `cluster` as the key for grouping.\n",
    "\n",
    "Once you complete the aggregation, save the cluster\n",
    "groupings to a file called `gan_cluster_final_agg.csv`.  You \n",
    "will have 12 rows and 33 columns in your data, representing\n",
    "the 12 clusters.\n",
    "\n",
    "\n",
    "\n",
    "### (25%) Visualize cluster data from GaN light pollution clustering. \n",
    "\n",
    "\n",
    "Now that we have clusters, we should be able \n",
    "to visualize it.  Right now, it is a bunch\n",
    "of numbers that might not make much sense --\n",
    "there are too many numbers!\n",
    "\n",
    "**&#167; Task:**  **Use the Seaborn library to display a heatmap of the data\n",
    "  in `gan_cluster_final_agg.csv`.**\n",
    "\n",
    "  In the `hw3_starter.ipynb` you may\n",
    "  notice `!pip install seaborn`. **Don't forget this step**.\n",
    "\n",
    "  You will have a heatmap that looks something like this:\n",
    "\n",
    "\n",
    "  <center>\n",
    "  ![](cluster_heatmap.png)\n",
    "  </center>\n",
    "\n",
    "\n",
    "  Notice the data in this heatmap is random, so **your**\n",
    "  heatmap will look very different, indeed!\n",
    "\n",
    "\n",
    "\n",
    "### (20%) Complete the online assessment. \n",
    "\n",
    "\n",
    "Please zip (or tar) the folder and subfolder for your assignment and submit it directly to Blackboard.\n",
    "\n",
    "Once you are done with the coding part of the assignment, you will need to complete the online assessment for\n",
    "the final 4 points (20%) of your grade.\n",
    "\n",
    "**&#167; Task:**  **Turn in your solution and complete the online HW3 assessment.**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:unidata-standard]",
   "language": "python",
   "name": "conda-env-unidata-standard-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": "1",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
